"""
This type stub file was generated by pyright.
"""

from typing import Any, Optional

"""Callbacks: utilities called at certain points during model training.
"""
_TRAIN = 'train'
_TEST = 'test'
_PREDICT = 'predict'
class CallbackList(object):
    """Container abstracting a list of callbacks.

    # Arguments
        callbacks: List of `Callback` instances.
        queue_length: Queue length for keeping
            running statistics over callback execution time.
    """
    def __init__(self, callbacks: Optional[Any] = ..., queue_length=...):
        self.callbacks = ...
        self.queue_length = ...
        self.params = ...
        self.model = ...
    
    def _reset_batch_timing(self):
        ...
    
    def append(self, callback):
        ...
    
    def set_params(self, params):
        self.params = ...
    
    def set_model(self, model):
        self.model = ...
    
    def _call_batch_hook(self, mode, hook, batch, logs: Optional[Any] = ...):
        """Helper function for all batch_{begin | end} methods."""
        ...
    
    def _call_begin_hook(self, mode):
        """Helper function for on_{train|test|predict}_begin methods."""
        ...
    
    def _call_end_hook(self, mode):
        """Helper function for on_{train|test|predict}_end methods."""
        ...
    
    def on_batch_begin(self, batch, logs: Optional[Any] = ...):
        ...
    
    def on_batch_end(self, batch, logs: Optional[Any] = ...):
        ...
    
    def on_epoch_begin(self, epoch, logs: Optional[Any] = ...):
        """Calls the `on_epoch_begin` methods of its callbacks.

        This function should only be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, Currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        """Calls the `on_epoch_end` methods of its callbacks.

        This function should only be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, metric results for this training epoch, and for the
                validation epoch if validation is performed. Validation result keys
                are prefixed with `val_`.
        """
        ...
    
    def on_train_batch_begin(self, batch, logs: Optional[Any] = ...):
        """Calls the `on_train_batch_begin` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        """
        ...
    
    def on_train_batch_end(self, batch, logs: Optional[Any] = ...):
        """Calls the `on_train_batch_end` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        """
        ...
    
    def on_test_batch_begin(self, batch, logs: Optional[Any] = ...):
        """Calls the `on_test_batch_begin` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        """
        ...
    
    def on_test_batch_end(self, batch, logs: Optional[Any] = ...):
        """Calls the `on_test_batch_end` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        """
        ...
    
    def on_predict_batch_begin(self, batch, logs: Optional[Any] = ...):
        """Calls the `on_predict_batch_begin` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        """
        ...
    
    def on_predict_batch_end(self, batch, logs: Optional[Any] = ...):
        """Calls the `on_predict_batch_end` methods of its callbacks.

        # Argument
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        """
        ...
    
    def on_train_begin(self, logs: Optional[Any] = ...):
        """Calls the `on_train_begin` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_train_end(self, logs: Optional[Any] = ...):
        """Calls the `on_train_end` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_test_begin(self, logs: Optional[Any] = ...):
        """Calls the `on_test_begin` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_test_end(self, logs: Optional[Any] = ...):
        """Calls the `on_test_end` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_predict_begin(self, logs: Optional[Any] = ...):
        """Calls the `on_predict_begin` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_predict_end(self, logs: Optional[Any] = ...):
        """Calls the `on_predict_end` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def __iter__(self):
        ...
    


class Callback(object):
    """Abstract base class used to build new callbacks.

    # Properties
        params: dict. Training parameters
            (eg. verbosity, batch size, number of epochs...).
        model: instance of `keras.models.Model`.
            Reference of the model being trained.

    The `logs` dictionary that callback methods
    take as argument will contain keys for quantities relevant to
    the current batch or epoch.

    Currently, the `.fit()` method of the `Sequential` model class
    will include the following quantities in the `logs` that
    it passes to its callbacks:

        on_epoch_end: logs include `acc` and `loss`, and
            optionally include `val_loss`
            (if validation is enabled in `fit`), and `val_acc`
            (if validation and accuracy monitoring are enabled).
        on_batch_begin: logs include `size`,
            the number of samples in the current batch.
        on_batch_end: logs include `loss`, and optionally `acc`
            (if accuracy monitoring is enabled).
    """
    def __init__(self):
        self.validation_data = ...
        self.model = ...
    
    def set_params(self, params):
        self.params = ...
    
    def set_model(self, model):
        self.model = ...
    
    def on_batch_begin(self, batch, logs: Optional[Any] = ...):
        """A backwards compatibility alias for `on_train_batch_begin`."""
        ...
    
    def on_batch_end(self, batch, logs: Optional[Any] = ...):
        """A backwards compatibility alias for `on_train_batch_end`."""
        ...
    
    def on_epoch_begin(self, epoch, logs: Optional[Any] = ...):
        """Called at the start of an epoch.

        Subclasses should override for any actions to run. This function should only
        be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        """Called at the end of an epoch.

        Subclasses should override for any actions to run. This function should only
        be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, metric results for this training epoch, and for the
                validation epoch if validation is performed. Validation result keys
                are prefixed with `val_`.
        """
        ...
    
    def on_train_batch_begin(self, batch, logs: Optional[Any] = ...):
        """Called at the beginning of a training batch in `fit` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        """
        ...
    
    def on_train_batch_end(self, batch, logs: Optional[Any] = ...):
        """Called at the end of a training batch in `fit` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        """
        ...
    
    def on_test_batch_begin(self, batch, logs: Optional[Any] = ...):
        """Called at the beginning of a batch in `evaluate` methods.

        Also called at the beginning of a validation batch in the `fit` methods,
        if validation data is provided.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        """
        ...
    
    def on_test_batch_end(self, batch, logs: Optional[Any] = ...):
        """Called at the end of a batch in `evaluate` methods.

        Also called at the end of a validation batch in the `fit` methods,
        if validation data is provided.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        """
        ...
    
    def on_predict_batch_begin(self, batch, logs: Optional[Any] = ...):
        """Called at the beginning of a batch in `predict` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        """
        ...
    
    def on_predict_batch_end(self, batch, logs: Optional[Any] = ...):
        """Called at the end of a batch in `predict` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        """
        ...
    
    def on_train_begin(self, logs: Optional[Any] = ...):
        """Called at the beginning of training.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_train_end(self, logs: Optional[Any] = ...):
        """Called at the end of training.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_test_begin(self, logs: Optional[Any] = ...):
        """Called at the beginning of evaluation or validation.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_test_end(self, logs: Optional[Any] = ...):
        """Called at the end of evaluation or validation.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_predict_begin(self, logs: Optional[Any] = ...):
        """Called at the beginning of prediction.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    
    def on_predict_end(self, logs: Optional[Any] = ...):
        """Called at the end of prediction.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        """
        ...
    


class BaseLogger(Callback):
    """Callback that accumulates epoch averages of metrics.

    This callback is automatically applied to every Keras model.

    # Arguments
        stateful_metrics: Iterable of string names of metrics that
            should *not* be averaged over an epoch.
            Metrics in this list will be logged as-is in `on_epoch_end`.
            All others will be averaged in `on_epoch_end`.
    """
    def __init__(self, stateful_metrics: Optional[Any] = ...):
        ...
    
    def on_epoch_begin(self, epoch, logs: Optional[Any] = ...):
        self.seen = ...
        self.totals = ...
    
    def on_batch_end(self, batch, logs: Optional[Any] = ...):
        ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    


class TerminateOnNaN(Callback):
    """Callback that terminates training when a NaN loss is encountered.
    """
    def on_batch_end(self, batch, logs: Optional[Any] = ...):
        ...
    


class ProgbarLogger(Callback):
    """Callback that prints metrics to stdout.

    # Arguments
        count_mode: One of "steps" or "samples".
            Whether the progress bar should
            count samples seen or steps (batches) seen.
        stateful_metrics: Iterable of string names of metrics that
            should *not* be averaged over an epoch.
            Metrics in this list will be logged as-is.
            All others will be averaged over time (e.g. loss, etc).

    # Raises
        ValueError: In case of invalid `count_mode`.
    """
    def __init__(self, count_mode=..., stateful_metrics: Optional[Any] = ...):
        ...
    
    def on_train_begin(self, logs: Optional[Any] = ...):
        self.verbose = ...
        self.epochs = ...
    
    def on_epoch_begin(self, epoch, logs: Optional[Any] = ...):
        self.seen = ...
    
    def on_batch_begin(self, batch, logs: Optional[Any] = ...):
        ...
    
    def on_batch_end(self, batch, logs: Optional[Any] = ...):
        ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    


class History(Callback):
    """Callback that records events into a `History` object.

    This callback is automatically applied to
    every Keras model. The `History` object
    gets returned by the `fit` method of models.
    """
    def on_train_begin(self, logs: Optional[Any] = ...):
        self.epoch = ...
        self.history = ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    


class ModelCheckpoint(Callback):
    """Save the model after every epoch.

    `filepath` can contain named formatting options,
    which will be filled with the values of `epoch` and
    keys in `logs` (passed in `on_epoch_end`).

    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,
    then the model checkpoints will be saved with the epoch number and
    the validation loss in the filename.

    # Arguments
        filepath: string, path to save the model file.
        monitor: quantity to monitor.
        verbose: verbosity mode, 0 or 1.
        save_best_only: if `save_best_only=True`,
            the latest best model according to
            the quantity monitored will not be overwritten.
        save_weights_only: if True, then only the model's weights will be
            saved (`model.save_weights(filepath)`), else the full model
            is saved (`model.save(filepath)`).
        mode: one of {auto, min, max}.
            If `save_best_only=True`, the decision
            to overwrite the current save file is made
            based on either the maximization or the
            minimization of the monitored quantity. For `val_acc`,
            this should be `max`, for `val_loss` this should
            be `min`, etc. In `auto` mode, the direction is
            automatically inferred from the name of the monitored quantity.
        period: Interval (number of epochs) between checkpoints.
    """
    def __init__(self, filepath, monitor=..., verbose=..., save_best_only: bool = ..., save_weights_only: bool = ..., mode=..., period=...):
        self.monitor = ...
        self.verbose = ...
        self.filepath = ...
        self.save_best_only = ...
        self.save_weights_only = ...
        self.period = ...
        self.epochs_since_last_save = ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    


class EarlyStopping(Callback):
    """Stop training when a monitored quantity has stopped improving.

    # Arguments
        monitor: quantity to be monitored.
        min_delta: minimum change in the monitored quantity
            to qualify as an improvement, i.e. an absolute
            change of less than min_delta, will count as no
            improvement.
        patience: number of epochs that produced the monitored
            quantity with no improvement after which training will
            be stopped.
            Validation quantities may not be produced for every
            epoch, if the validation frequency
            (`model.fit(validation_freq=5)`) is greater than one.
        verbose: verbosity mode.
        mode: one of {auto, min, max}. In `min` mode,
            training will stop when the quantity
            monitored has stopped decreasing; in `max`
            mode it will stop when the quantity
            monitored has stopped increasing; in `auto`
            mode, the direction is automatically inferred
            from the name of the monitored quantity.
        baseline: Baseline value for the monitored quantity to reach.
            Training will stop if the model doesn't show improvement
            over the baseline.
        restore_best_weights: whether to restore model weights from
            the epoch with the best value of the monitored quantity.
            If False, the model weights obtained at the last step of
            training are used.
    """
    def __init__(self, monitor=..., min_delta=..., patience=..., verbose=..., mode=..., baseline: Optional[Any] = ..., restore_best_weights: bool = ...):
        self.monitor = ...
        self.baseline = ...
        self.patience = ...
        self.verbose = ...
        self.min_delta = ...
        self.wait = ...
        self.stopped_epoch = ...
        self.restore_best_weights = ...
        self.best_weights = ...
    
    def on_train_begin(self, logs: Optional[Any] = ...):
        self.wait = ...
        self.stopped_epoch = ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    
    def on_train_end(self, logs: Optional[Any] = ...):
        ...
    
    def get_monitor_value(self, logs):
        ...
    


class RemoteMonitor(Callback):
    """Callback used to stream events to a server.

    Requires the `requests` library.
    Events are sent to `root + '/publish/epoch/end/'` by default. Calls are
    HTTP POST, with a `data` argument which is a
    JSON-encoded dictionary of event data.
    If send_as_json is set to True, the content type of the request will be
    application/json. Otherwise the serialized JSON will be send within a form

    # Arguments
        root: String; root url of the target server.
        path: String; path relative to `root` to which the events will be sent.
        field: String; JSON field under which the data will be stored.
            The field is used only if the payload is sent within a form
            (i.e. send_as_json is set to False).
        headers: Dictionary; optional custom HTTP headers.
        send_as_json: Boolean; whether the request should be send as
            application/json.
    """
    def __init__(self, root=..., path=..., field=..., headers: Optional[Any] = ..., send_as_json: bool = ...):
        self.root = ...
        self.path = ...
        self.field = ...
        self.headers = ...
        self.send_as_json = ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    


class LearningRateScheduler(Callback):
    """Learning rate scheduler.

    # Arguments
        schedule: a function that takes an epoch index as input
            (integer, indexed from 0) and current learning rate
            and returns a new learning rate as output (float).
        verbose: int. 0: quiet, 1: update messages.
    """
    def __init__(self, schedule, verbose=...):
        self.schedule = ...
        self.verbose = ...
    
    def on_epoch_begin(self, epoch, logs: Optional[Any] = ...):
        ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    


class ReduceLROnPlateau(Callback):
    """Reduce learning rate when a metric has stopped improving.

    Models often benefit from reducing the learning rate by a factor
    of 2-10 once learning stagnates. This callback monitors a
    quantity and if no improvement is seen for a 'patience' number
    of epochs, the learning rate is reduced.

    # Example

    ```python
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                                  patience=5, min_lr=0.001)
    model.fit(X_train, Y_train, callbacks=[reduce_lr])
    ```

    # Arguments
        monitor: quantity to be monitored.
        factor: factor by which the learning rate will
            be reduced. new_lr = lr * factor
        patience: number of epochs that produced the monitored
            quantity with no improvement after which training will
            be stopped.
            Validation quantities may not be produced for every
            epoch, if the validation frequency
            (`model.fit(validation_freq=5)`) is greater than one.
        verbose: int. 0: quiet, 1: update messages.
        mode: one of {auto, min, max}. In `min` mode,
            lr will be reduced when the quantity
            monitored has stopped decreasing; in `max`
            mode it will be reduced when the quantity
            monitored has stopped increasing; in `auto`
            mode, the direction is automatically inferred
            from the name of the monitored quantity.
        min_delta: threshold for measuring the new optimum,
            to only focus on significant changes.
        cooldown: number of epochs to wait before resuming
            normal operation after lr has been reduced.
        min_lr: lower bound on the learning rate.
    """
    def __init__(self, monitor=..., factor=..., patience=..., verbose=..., mode=..., min_delta=..., cooldown=..., min_lr=..., **kwargs):
        self.monitor = ...
        self.factor = ...
        self.min_lr = ...
        self.min_delta = ...
        self.patience = ...
        self.verbose = ...
        self.cooldown = ...
        self.cooldown_counter = ...
        self.wait = ...
        self.best = ...
        self.mode = ...
        self.monitor_op = ...
    
    def _reset(self):
        """Resets wait counter and cooldown counter.
        """
        self.cooldown_counter = ...
        self.wait = ...
    
    def on_train_begin(self, logs: Optional[Any] = ...):
        ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    
    def in_cooldown(self):
        ...
    


class CSVLogger(Callback):
    """Callback that streams epoch results to a csv file.

    Supports all values that can be represented as a string,
    including 1D iterables such as np.ndarray.

    # Example

    ```python
    csv_logger = CSVLogger('training.log')
    model.fit(X_train, Y_train, callbacks=[csv_logger])
    ```

    # Arguments
        filename: filename of the csv file, e.g. 'run/log.csv'.
        separator: string used to separate elements in the csv file.
        append: True: append if file exists (useful for continuing
            training). False: overwrite existing file,
    """
    def __init__(self, filename, separator=..., append: bool = ...):
        self.sep = ...
        self.filename = ...
        self.append = ...
        self.writer = ...
        self.keys = ...
        self.append_header = ...
    
    def on_train_begin(self, logs: Optional[Any] = ...):
        self.csv_file = ...
    
    def on_epoch_end(self, epoch, logs: Optional[Any] = ...):
        ...
    
    def on_train_end(self, logs: Optional[Any] = ...):
        self.writer = ...
    
    def __del__(self):
        ...
    


class LambdaCallback(Callback):
    r"""Callback for creating simple, custom callbacks on-the-fly.

    This callback is constructed with anonymous functions that will be called
    at the appropriate time. Note that the callbacks expects positional
    arguments, as:

     - `on_epoch_begin` and `on_epoch_end` expect two positional arguments:
        `epoch`, `logs`
     - `on_batch_begin` and `on_batch_end` expect two positional arguments:
        `batch`, `logs`
     - `on_train_begin` and `on_train_end` expect one positional argument:
        `logs`

    # Arguments
        on_epoch_begin: called at the beginning of every epoch.
        on_epoch_end: called at the end of every epoch.
        on_batch_begin: called at the beginning of every batch.
        on_batch_end: called at the end of every batch.
        on_train_begin: called at the beginning of model training.
        on_train_end: called at the end of model training.

    # Example

    ```python
    # Print the batch number at the beginning of every batch.
    batch_print_callback = LambdaCallback(
        on_batch_begin=lambda batch,logs: print(batch))

    # Stream the epoch loss to a file in JSON format. The file content
    # is not well-formed JSON but rather has a JSON object per line.
    import json
    json_log = open('loss_log.json', mode='wt', buffering=1)
    json_logging_callback = LambdaCallback(
        on_epoch_end=lambda epoch, logs: json_log.write(
            json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\n'),
        on_train_end=lambda logs: json_log.close()
    )

    # Terminate some processes after having finished model training.
    processes = ...
    cleanup_callback = LambdaCallback(
        on_train_end=lambda logs: [
            p.terminate() for p in processes if p.is_alive()])

    model.fit(...,
              callbacks=[batch_print_callback,
                         json_logging_callback,
                         cleanup_callback])
    ```
    """
    def __init__(self, on_epoch_begin: Optional[Any] = ..., on_epoch_end: Optional[Any] = ..., on_batch_begin: Optional[Any] = ..., on_batch_end: Optional[Any] = ..., on_train_begin: Optional[Any] = ..., on_train_end: Optional[Any] = ..., **kwargs):
        ...
    


