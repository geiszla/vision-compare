"""
This type stub file was generated by pyright.
"""

from typing import Any, Optional

"""Model saving utilities.
"""
def _uniquify(names):
    """Uniquify list of strings.

    Custom layers and optimizers written by users
    for TF 1.x might produce weights with same variable
    names in TF 2. This method "uniquifies" a given list
    of names.

    e.g: `['a', 'b', 'b', 'c'] -> ['a', 'b', 'b_2', 'c']`

    # Arguments
        names: List of strings.

    # Returns
        List of unique strings.
    """
    ...

def _serialize_model(model, h5dict, include_optimizer: bool = ...):
    """Model serialization logic.

    This method is used for both writing to HDF5 file/group,
    as well as pickling. This is achieved via a
    `keras.utils.hdf5_utls.H5Dict` object, which can wrap HDF5
    files, groups and dicts with a common API.

    # Arguments
        model: Keras model instance to be serialized.
        h5dict: keras.utils.io_utils.HD5Dict instance.
        include_optimizer: If True, serialize optimizer's state together.

    """
    ...

def _deserialize_model(h5dict, custom_objects: Optional[Any] = ..., compile: bool = ...):
    """De-serializes a model serialized via _serialize_model

    # Arguments
        h5dict: `keras.utils.hdf5_utils.HFDict` instance.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.
        compile: Boolean, whether to compile the model
            after loading.

    # Returns
        A Keras model instance. If an optimizer was found
        as part of the saved model, the model is already
        compiled. Otherwise, the model is uncompiled and
        a warning will be displayed. When `compile` is set
        to False, the compilation is omitted without any
        warning.
    """
    ...

def _gcs_copy(source_filepath, target_filepath, overwrite: bool = ...):
    """Copies a file to/from/within Google Cloud Storage (GCS).

    # Arguments
        source_filepath: String, path to the file on filesystem or object on GCS to
            copy from.
        target_filepath: String, path to the file on filesystem or object on GCS to
            copy to.
        overwrite: Whether we should overwrite an existing file/object at the target
            location, or instead ask the user with a manual prompt.
    """
    ...

def _is_gcs_location(filepath):
    """Checks if `filepath` is referencing a google storage bucket.

    # Arguments
        filepath: The location to check.
    """
    ...

def allow_write_to_gcs(save_function):
    """Function decorator to support saving to Google Cloud Storage (GCS).

    This decorator parses the `filepath` argument of the `save_function` and
    transfers the file to GCS if `filepath` starts with "gs://".

    Note: the file is temporarily writen to local filesystem before copied to GSC.

    # Arguments
        save_function: The function to wrap, with requirements:
            - second positional argument should indicate the location to save to.
            - third positional argument should be the `overwrite` option indicating
            whether we should overwrite an existing file/object at the target
            location, or instead ask the user with a manual prompt.
    """
    ...

def allow_read_from_gcs(load_function):
    """Function decorator to support loading from Google Cloud Storage (GCS).

    This decorator parses the `filepath` argument of the `load_function` and
    fetches the required object from GCS if `filepath` starts with "gs://".

    Note: the file is temporarily copied to local filesystem from GCS before loaded.

    # Arguments
        load_function: The function to wrap, with requirements:
            - should have one _named_ argument `filepath` indicating the location to
            load from.
    """
    ...

@allow_write_to_gcs
def save_model(model, filepath, overwrite: bool = ..., include_optimizer: bool = ...):
    """Save a model to a HDF5 file.

    Note: Please also see
    [How can I install HDF5 or h5py to save my models in Keras?](
        /getting-started/faq/
        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)
    in the FAQ for instructions on how to install `h5py`.

    The saved model contains:
        - the model's configuration (topology)
        - the model's weights
        - the model's optimizer's state (if any)

    Thus the saved model can be reinstantiated in
    the exact same state, without any of the code
    used for model definition or training.

    # Arguments
        model: Keras model instance to be saved.
        filepath: one of the following:
            - string, path to the file to save the model to
            - h5py.File or h5py.Group object where to save the model
            - any file-like object implementing the method `write` that accepts
                `bytes` data (e.g. `io.BytesIO`).
        overwrite: Whether we should overwrite any existing
            model at the target location, or instead
            ask the user with a manual prompt.
        include_optimizer: If True, save optimizer's state together.

    # Raises
        ImportError: if h5py is not available.
    """
    ...

@allow_read_from_gcs
def load_model(filepath, custom_objects: Optional[Any] = ..., compile: bool = ...):
    """Loads a model saved via `save_model`.

    # Arguments
        filepath: one of the following:
            - string, path to the saved model
            - h5py.File or h5py.Group object from which to load the model
            - any file-like object implementing the method `read` that returns
            `bytes` data (e.g. `io.BytesIO`) that represents a valid h5py file image.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.
        compile: Boolean, whether to compile the model
            after loading.

    # Returns
        A Keras model instance. If an optimizer was found
        as part of the saved model, the model is already
        compiled. Otherwise, the model is uncompiled and
        a warning will be displayed. When `compile` is set
        to False, the compilation is omitted without any
        warning.

    # Raises
        ImportError: if h5py is not available.
        ValueError: In case of an invalid savefile.
    """
    ...

def pickle_model(model):
    ...

def unpickle_model(state):
    ...

def model_from_config(config, custom_objects: Optional[Any] = ...):
    """Instantiates a Keras model from its config.

    # Arguments
        config: Configuration dictionary.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.

    # Returns
        A Keras model instance (uncompiled).

    # Raises
        TypeError: if `config` is not a dictionary.
    """
    ...

def model_from_yaml(yaml_string, custom_objects: Optional[Any] = ...):
    """Parses a yaml model configuration file and returns a model instance.

    # Arguments
        yaml_string: YAML string encoding a model configuration.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.

    # Returns
        A Keras model instance (uncompiled).
    """
    ...

def model_from_json(json_string, custom_objects: Optional[Any] = ...):
    """Parses a JSON model configuration file and returns a model instance.

    # Arguments
        json_string: JSON string encoding a model configuration.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.

    # Returns
        A Keras model instance (uncompiled).
    """
    ...

def save_attributes_to_hdf5_group(group, name, data):
    """Saves attributes (data) of the specified name into the HDF5 group.

    This method deals with an inherent problem of HDF5 file which is not
    able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.

    # Arguments
        group: A pointer to a HDF5 group.
        name: A name of the attributes to save.
        data: Attributes data to store.
    """
    ...

def load_attributes_from_hdf5_group(group, name):
    """Loads attributes of the specified name from the HDF5 group.

    This method deals with an inherent problem
    of HDF5 file which is not able to store
    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.

    # Arguments
        group: A pointer to a HDF5 group.
        name: A name of the attributes to load.

    # Returns
        data: Attributes data.
    """
    ...

def save_weights_to_hdf5_group(group, layers):
    """Saves weights into the HDF5 group.

    # Arguments
        group: A pointer to a HDF5 group.
        layers: Layers to load.
    """
    ...

def preprocess_weights_for_loading(layer, weights, original_keras_version: Optional[Any] = ..., original_backend: Optional[Any] = ..., reshape: bool = ...):
    """Converts layers weights from Keras 1 format to Keras 2.

    # Arguments
        layer: Layer instance.
        weights: List of weights values (Numpy arrays).
        original_keras_version: Keras version for the weights, as a string.
        original_backend: Keras backend the weights were trained with,
            as a string.
        reshape: Reshape weights to fit the layer when the correct number
            of values are present but the shape does not match.

    # Returns
        A list of weights values (Numpy arrays).
    """
    ...

def _convert_rnn_weights(layer, weights):
    """Converts weights for RNN layers between native and CuDNN format.

    Input kernels for each gate are transposed and converted between Fortran
    and C layout, recurrent kernels are transposed. For LSTM biases are summed/
    split in half, for GRU biases are reshaped.

    Weights can be converted in both directions between `LSTM` and`CuDNNSLTM`
    and between `CuDNNGRU` and `GRU(reset_after=True)`. Default `GRU` is not
    compatible with `CuDNNGRU`.

    For missing biases in `LSTM`/`GRU` (`use_bias=False`),
    no conversion is made.

    # Arguments
        layer: Target layer instance.
        weights: List of source weights values (input kernels, recurrent
            kernels, [biases]) (Numpy arrays).

    # Returns
        A list of converted weights values (Numpy arrays).

    # Raises
        ValueError: for incompatible GRU layer/weights or incompatible biases
    """
    ...

def _need_convert_kernel(original_backend):
    """Checks if conversion on kernel matrices is required during weight loading.

    The convolution operation is implemented differently in different backends.
    While TH implements convolution, TF and CNTK implement the correlation operation.
    So the channel axis needs to be flipped when TF weights are loaded on a TH model,
    or vice versa. However, there's no conversion required between TF and CNTK.

    # Arguments
        original_backend: Keras backend the weights were trained with, as a string.

    # Returns
        `True` if conversion on kernel matrices is required, otherwise `False`.
    """
    ...

def load_weights_from_hdf5_group(f, layers, reshape: bool = ...):
    """Implements topological (order-based) weight loading.

    # Arguments
        f: A pointer to a HDF5 group.
        layers: a list of target layers.
        reshape: Reshape weights to fit the layer when the correct number
            of values are present but the shape does not match.

    # Raises
        ValueError: in case of mismatch between provided layers
            and weights file.
    """
    ...

def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch: bool = ..., reshape: bool = ...):
    """Implements name-based weight loading.

    (instead of topological weight loading).

    Layers that have no matching name are skipped.

    # Arguments
        f: A pointer to a HDF5 group.
        layers: A list of target layers.
        skip_mismatch: Boolean, whether to skip loading of layers
            where there is a mismatch in the number of weights,
            or a mismatch in the shape of the weights.
        reshape: Reshape weights to fit the layer when the correct number
            of values are present but the shape does not match.

    # Raises
        ValueError: in case of mismatch between provided layers
            and weights file and skip_mismatch=False.
    """
    ...

