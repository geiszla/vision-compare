"""
This type stub file was generated by pyright.
"""

from collections import defaultdict
from contextlib import contextmanager
from theano import tensor as T
from typing import Any, Optional

py_all = all
py_any = any
py_sum = sum
py_slice = slice
_LEARNING_PHASE = T.scalar(dtype='uint8', name='keras_learning_phase')
_UID_PREFIXES = defaultdict(int)
def learning_phase():
    ...

def set_learning_phase(value):
    ...

def get_uid(prefix=...):
    """Provides a unique UID given a string prefix.

    # Arguments
        prefix: string.

    # Returns
        An integer.

    # Example
    ```python
        >>> keras.backend.get_uid('dense')
        1
        >>> keras.backend.get_uid('dense')
        2
    ```

    """
    ...

def reset_uids():
    ...

def _assert_sparse_module():
    ...

def is_sparse(tensor):
    ...

def to_dense(tensor):
    ...

NAME_SCOPE_STACK = []
@contextmanager
def name_scope(name):
    ...

def _prepare_name(name, default):
    ...

def variable(value, dtype: Optional[Any] = ..., name: Optional[Any] = ..., constraint: Optional[Any] = ...):
    """Instantiates a variable and returns it.

    # Arguments
        value: Numpy array, initial value of the tensor.
        dtype: Tensor type.
        name: Optional name string for the tensor.
        constraint: Optional projection function to be
            applied to the variable after an optimizer update.

    # Returns
        A variable instance (with Keras metadata included).
    """
    ...

def is_variable(x):
    ...

def constant(value, dtype: Optional[Any] = ..., shape: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def is_keras_tensor(x):
    """Returns whether `x` is a Keras tensor.

    A "Keras tensor" is a tensor that was returned by a Keras layer,
    (`Layer` class) or by `Input`.

    # Arguments
        x: A candidate tensor.

    # Returns
        A boolean: Whether the argument is a Keras tensor.

    # Raises
        ValueError: In case `x` is not a symbolic tensor.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> from keras.layers import Input, Dense
        >>> np_var = numpy.array([1, 2])
        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
        ValueError
        >>> k_var = tf.placeholder('float32', shape=(1,1))
        >>> # A variable indirectly created outside of keras is not a Keras tensor.
        >>> K.is_keras_tensor(k_var)
        False
        >>> keras_var = K.variable(np_var)
        >>> # A variable created with the keras backend is not a Keras tensor.
        >>> K.is_keras_tensor(keras_var)
        False
        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))
        >>> # A placeholder is not a Keras tensor.
        >>> K.is_keras_tensor(keras_placeholder)
        False
        >>> keras_input = Input([10])
        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
        True
        >>> keras_layer_output = Dense(10)(keras_input)
        >>> # Any Keras layer output is a Keras tensor.
        >>> K.is_keras_tensor(keras_layer_output)
        True
    ```
    """
    ...

def is_tensor(x):
    ...

def placeholder(shape: Optional[Any] = ..., ndim: Optional[Any] = ..., dtype: Optional[Any] = ..., sparse: bool = ..., name: Optional[Any] = ...):
    """Instantiate an input data placeholder variable.
    """
    ...

def is_placeholder(x):
    """Returns whether `x` is a placeholder.

    # Arguments
        x: A candidate placeholder.

    # Returns
        Boolean.
    """
    ...

def shape(x):
    """Returns the shape of a tensor.

    Warning: type returned will be different for
    Theano backend (Theano tensor type) and TF backend (TF TensorShape).
    """
    ...

def int_shape(x):
    """Returns the shape of a Keras tensor or a Keras variable as a tuple of
    integers or None entries.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tuple of integers (or None entries).
    """
    ...

def ndim(x):
    ...

def dtype(x):
    ...

def eval(x):
    """Returns the value of a tensor.
    """
    ...

def zeros(shape, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    """Instantiates an all-zeros variable.
    """
    ...

def ones(shape, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    """Instantiates an all-ones variable.
    """
    ...

def eye(size, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    """Instantiates an identity matrix.
    """
    ...

def ones_like(x, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def zeros_like(x, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def identity(x, name: Optional[Any] = ...):
    """Returns a tensor with the same content as the input tensor.

    # Arguments
        x: The input tensor.
        name: String, name for the variable to create.

    # Returns
        A tensor of the same shape, type and content.
    """
    ...

def random_uniform_variable(shape, low, high, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def random_normal_variable(shape, mean, scale, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def count_params(x):
    """Returns the number of scalars in a tensor.

    Return: numpy integer.
    """
    ...

def cast(x, dtype):
    ...

def size(x, name: Optional[Any] = ...):
    """Returns the size of a tensor.
    # Arguments
        x: The input tensor.
        name: A name for the operation (optional).
    # Returns
        Size of the tensor.
    ```
    """
    ...

def update(x, new_x):
    ...

def update_add(x, increment):
    ...

def update_sub(x, decrement):
    ...

def moving_average_update(variable, value, momentum):
    ...

def dot(x, y):
    ...

def batch_dot(x, y, axes: Optional[Any] = ...):
    """Batchwise dot product.

    batch_dot results in a tensor with less dimensions than the input.
    If the number of dimensions is reduced to 1, we use `expand_dims` to
    make sure that ndim is at least 2.

    # Arguments
        x, y: tensors with ndim >= 2
        axes: list (or single) int with target dimensions

    # Returns
        A tensor with shape equal to the concatenation of x's shape
        (less the dimension that was summed over) and y's shape
        (less the batch dimension and the dimension that was summed over).
        If the final rank is 1, we reshape it to (batch_size, 1).

    # Examples
        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]
        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal
        of x.dot(y.T), although we never have to calculate the off-diagonal
        elements.

        Shape inference:
        Let x's shape be (100, 20) and y's shape be (100, 30, 20).
        If dot_axes is (1, 2), to find the output shape of resultant tensor,
            loop through each dimension in x's shape and y's shape:
        x.shape[0] : 100 : append to output shape
        x.shape[1] : 20 : do not append to output shape,
            dimension 1 of x has been summed over. (dot_axes[0] = 1)
        y.shape[0] : 100 : do not append to output shape,
            always ignore first dimension of y
        y.shape[1] : 30 : append to output shape
        y.shape[2] : 20 : do not append to output shape,
            dimension 2 of y has been summed over. (dot_axes[1] = 2)

        output_shape = (100, 30)
    """
    ...

def transpose(x):
    ...

def gather(reference, indices):
    """Retrieves the elements of indices `indices` in the tensor `reference`.

    # Arguments
        reference: A tensor.
        indices: An integer tensor of indices.

    # Returns
        A tensor of same type as `reference`.
    """
    ...

def max(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def min(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def sum(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    """Sum of the values in a tensor, alongside the specified axis.
    """
    ...

def prod(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    """Multiply the values in a tensor, alongside the specified axis.
    """
    ...

def cumsum(x, axis=...):
    """Cumulative sum of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to compute the sum.

    # Returns
        A tensor of the cumulative sum of values of `x` along `axis`.
    """
    ...

def cumprod(x, axis=...):
    """Cumulative product of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to compute the product.

    # Returns
        A tensor of the cumulative product of values of `x` along `axis`.
    """
    ...

def mean(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    """Mean of a tensor, alongside the specified axis.
    """
    ...

def std(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def var(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def any(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    """Bitwise reduction (logical OR).
    """
    ...

def all(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    """Bitwise reduction (logical AND).
    """
    ...

def _set_keras_shape_for_reduction(x, y, axis, keepdims):
    ...

def argmax(x, axis=...):
    ...

def argmin(x, axis=...):
    ...

def square(x):
    ...

def abs(x):
    ...

def sqrt(x):
    ...

def exp(x):
    ...

def log(x):
    ...

def logsumexp(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    """Computes log(sum(exp(elements across dimensions of a tensor))).

    This function is more numerically stable than log(sum(exp(x))).
    It avoids overflows caused by taking the exp of large inputs and
    underflows caused by taking the log of small inputs.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to reduce over.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`, the reduced dimension is
            retained with length 1.

    # Returns
        The reduced tensor.
    """
    ...

def round(x):
    ...

def sign(x):
    ...

def pow(x, a):
    ...

def clip(x, min_value, max_value):
    ...

def equal(x, y):
    ...

def not_equal(x, y):
    ...

def greater(x, y):
    ...

def greater_equal(x, y):
    ...

def less(x, y):
    ...

def less_equal(x, y):
    ...

def maximum(x, y):
    ...

def minimum(x, y):
    ...

def sin(x):
    ...

def cos(x):
    ...

def normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=...):
    """Computes mean and std for batch then apply batch_normalization on batch.
    """
    ...

def batch_normalization(x, mean, var, beta, gamma, axis=..., epsilon=...):
    """Apply batch normalization on x given mean, var, beta and gamma.
    """
    ...

def _old_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=...):
    """Computes mean and std for batch then apply batch_normalization on batch.
    """
    ...

def _old_batch_normalization(x, mean, var, beta, gamma, epsilon=...):
    """Apply batch normalization on x given mean, var, beta and gamma.
    """
    ...

def concatenate(tensors, axis=...):
    ...

def reshape(x, shape):
    ...

def permute_dimensions(x, pattern):
    """Transpose dimensions.

    pattern should be a tuple or list of
    dimension indices, e.g. [0, 2, 1].
    """
    ...

def repeat_elements(x, rep, axis):
    """Repeat the elements of a tensor along an axis, like np.repeat.

    If x has shape (s1, s2, s3) and axis=1, the output
    will have shape (s1, s2 * rep, s3).
    """
    ...

def resize_images(x, height_factor, width_factor, data_format, interpolation=...):
    """Resize the images contained in a 4D tensor of shape
    - [batch, channels, height, width] (for 'channels_first' data_format)
    - [batch, height, width, channels] (for 'channels_last' data_format)
    by a factor of (height_factor, width_factor). Both factors should be
    positive integers.
    """
    ...

def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):
    """Resize the volume contained in a 5D tensor of shape
    - [batch, channels, depth, height, width] (for 'channels_first' data_format)
    - [batch, depth, height, width, channels] (for 'channels_last' data_format)
    by a factor of (depth_factor, height_factor, width_factor).
    Both factors should be positive integers.
    """
    ...

def repeat(x, n):
    """Repeat a 2D tensor.

    If x has shape (samples, dim) and n=2,
    the output will have shape (samples, 2, dim).
    """
    ...

def arange(start, stop: Optional[Any] = ..., step=..., dtype=...):
    """Creates a 1-D tensor containing a sequence of integers.

    The function arguments use the same convention as
    Theano's arange: if only one argument is provided,
    it is in fact the "stop" argument.

    The default type of the returned tensor is 'int32' to
    match TensorFlow's default.
    """
    ...

def tile(x, n):
    ...

def flatten(x):
    ...

def batch_flatten(x):
    """Turn a n-D tensor into a 2D tensor where
    the first dimension is conserved.
    """
    ...

def expand_dims(x, axis=...):
    """Add a 1-sized dimension at index "dim".
    """
    ...

def squeeze(x, axis):
    """Remove a 1-dimension from the tensor at index "axis".
    """
    ...

def temporal_padding(x, padding=...):
    """Pad the middle dimension of a 3D tensor
    with "padding" zeros left and right.

    Apologies for the inane API, but Theano makes this
    really hard.
    """
    ...

def spatial_2d_padding(x, padding=..., data_format: Optional[Any] = ...):
    """Pad the 2nd and 3rd dimensions of a 4D tensor
    with "padding[0]" and "padding[1]" (resp.) zeros left and right.
    """
    ...

def spatial_3d_padding(x, padding=..., data_format: Optional[Any] = ...):
    """Pad the 2nd, 3rd and 4th dimensions of a 5D tensor
    with "padding[0]", "padding[1]" and "padding[2]" (resp.) zeros left and right.
    """
    ...

def stack(x, axis=...):
    ...

def one_hot(indices, num_classes):
    """Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))
    Output: (n + 1)D one hot representation of the input
    with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)
    """
    ...

def reverse(x, axes):
    """Reverse a tensor along the specified axes
    """
    ...

def slice(x, start, size):
    ...

def pattern_broadcast(x, broadcastable):
    ...

def get_value(x):
    ...

def batch_get_value(xs):
    """Returns the value of more than one tensor variable,
    as a list of Numpy arrays.
    """
    ...

def set_value(x, value):
    ...

def batch_set_value(tuples):
    ...

def get_variable_shape(x):
    ...

def print_tensor(x, message=...):
    """Print the message & the tensor when evaluated & return the same tensor.
    """
    ...

class Function(object):
    def __init__(self, inputs, outputs, updates=..., name: Optional[Any] = ..., **kwargs):
        self.outputs = ...
        self.function = ...
        self.name = ...
    
    def __call__(self, inputs):
        ...
    


def _raise_invalid_arg(key):
    ...

def function(inputs, outputs, updates=..., **kwargs):
    ...

def gradients(loss, variables):
    ...

def stop_gradient(variables):
    """Returns `variables` but with zero gradient w.r.t. every other variable.

    # Arguments
        variables: tensor or list of tensors to consider constant with respect
            to any other variable.

    # Returns
        A single tensor or a list of tensors (depending on the passed argument)
            that has constant gradient with respect to any other variable.
    """
    ...

def rnn(step_function, inputs, initial_states, go_backwards: bool = ..., mask: Optional[Any] = ..., constants: Optional[Any] = ..., unroll: bool = ..., input_length: Optional[Any] = ...):
    """Iterates over the time dimension of a tensor.

    # Arguments
        step_function:
            Parameters:
                inputs: Tensor with shape (samples, ...) (no time dimension),
                    representing input for the batch of samples at a certain
                    time step.
                states: List of tensors.
            Returns:
                outputs: Tensor with shape (samples, ...) (no time dimension),
                new_states: List of tensors, same length and shapes
                    as 'states'.
        inputs: Tensor of temporal data of shape (samples, time, ...)
            (at least 3D).
        initial_states: Tensor with shape (samples, ...) (no time dimension),
            containing the initial values for the states used in
            the step function.
        go_backwards: Boolean. If True, do the iteration over the time
            dimension in reverse order and return the reversed sequence.
        mask: Binary tensor with shape (samples, time),
            with a zero for every element that is masked.
        constants: A list of constant values passed at each step.
        unroll: Whether to unroll the RNN or to use a symbolic loop
            (`while_loop` or `scan` depending on backend).
        input_length: Static number of timesteps in the input.
            Must be specified if using `unroll`.

    # Returns
        A tuple (last_output, outputs, new_states).

        last_output: The latest output of the rnn, of shape `(samples, ...)`
        outputs: Tensor with shape `(samples, time, ...)` where each
            entry `outputs[s, t]` is the output of the step function
            at time `t` for sample `s`.
        new_states: List of tensors, latest states returned by
            the step function, of shape `(samples, ...)`.
    """
    ...

def switch(condition, then_expression, else_expression):
    """Switches between two operations depending on a scalar value.

    Note that both `then_expression` and `else_expression`
    should be symbolic tensors of the *same shape*.

    # Arguments
        condition: scalar tensor (`int` or `bool`).
        then_expression: either a tensor, or a callable that returns a tensor.
        else_expression: either a tensor, or a callable that returns a tensor.

    # Returns
        The selected tensor.
    """
    ...

def in_train_phase(x, alt, training: Optional[Any] = ...):
    """Selects `x` in train phase, and `alt` otherwise.

    Note that `alt` should have the *same shape* as `x`.

    # Returns
        Either `x` or `alt` based on the `training` flag.
        the `training` flag defaults to `K.learning_phase()`.
    """
    ...

def in_test_phase(x, alt, training: Optional[Any] = ...):
    """Selects `x` in test phase, and `alt` otherwise.
    Note that `alt` should have the *same shape* as `x`.

    # Returns
        Either `x` or `alt` based on `K.learning_phase`.
    """
    ...

def _assert_has_capability(module, func):
    ...

def elu(x, alpha=...):
    """ Exponential linear unit

    # Arguments
        x: Tensor to compute the activation function for.
        alpha: scalar
    """
    ...

def relu(x, alpha=..., max_value: Optional[Any] = ..., threshold=...):
    ...

def softmax(x, axis=...):
    ...

def softplus(x):
    ...

def softsign(x):
    ...

def categorical_crossentropy(target, output, from_logits: bool = ..., axis=...):
    ...

def sparse_categorical_crossentropy(target, output, from_logits: bool = ..., axis=...):
    ...

def binary_crossentropy(target, output, from_logits: bool = ...):
    ...

def sigmoid(x):
    ...

def hard_sigmoid(x):
    ...

def tanh(x):
    ...

def dropout(x, level, noise_shape: Optional[Any] = ..., seed: Optional[Any] = ...):
    """Sets entries in `x` to zero at random,
    while scaling the entire tensor.

    # Arguments
        x: tensor
        level: fraction of the entries in the tensor
            that will be set to 0.
        noise_shape: shape for randomly generated keep/drop flags,
            must be broadcastable to the shape of `x`
        seed: random seed to ensure determinism.
    """
    ...

def l2_normalize(x, axis: Optional[Any] = ...):
    ...

def in_top_k(predictions, targets, k):
    """Returns whether the `targets` are in the top `k` `predictions`.

    # Arguments
        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.
        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.
        k: An `int`, number of top elements to consider.

    # Returns
        A 1D tensor of length `batch_size` and type `bool`.
        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`
        values of `predictions[i]`.
    """
    ...

def _preprocess_conv2d_input(x, data_format):
    ...

def _preprocess_conv3d_input(x, data_format):
    ...

def _preprocess_conv2d_kernel(kernel, data_format):
    ...

def _preprocess_conv2d_depthwise_kernel(kernel, kernel_shape, data_format):
    ...

def _preprocess_conv3d_kernel(kernel, data_format):
    ...

def _preprocess_padding(padding):
    ...

def _preprocess_conv2d_image_shape(image_shape, data_format):
    ...

def _preprocess_conv3d_volume_shape(volume_shape, data_format):
    ...

def _preprocess_conv2d_filter_shape(filter_shape, data_format):
    ...

def _preprocess_conv2d_depthwise_filter_shape(filter_shape, data_format):
    ...

def _preprocess_conv3d_filter_shape(filter_shape, data_format):
    ...

def _postprocess_conv2d_output(conv_out, x, padding, kernel_shape, strides, data_format):
    ...

def _postprocess_conv3d_output(conv_out, x, padding, kernel_shape, strides, data_format):
    ...

def conv1d(x, kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """1D convolution.

    # Arguments
        kernel: kernel tensor.
        strides: stride integer.
        padding: string, `"same"`, `"causal"` or `"valid"`.
        data_format: string, one of "channels_last", "channels_first"
        dilation_rate: integer.
    """
    ...

def conv2d(x, kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """2D convolution.

    # Arguments
        kernel: kernel tensor.
        strides: strides tuple.
        padding: string, "same" or "valid".
        data_format: "channels_last" or "channels_first".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.
    """
    ...

def conv2d_transpose(x, kernel, output_shape, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """2D deconvolution (transposed convolution).

    # Arguments
        kernel: kernel tensor.
        output_shape: desired dimensions of output.
        strides: strides tuple.
        padding: string, "same" or "valid".
        data_format: "channels_last" or "channels_first".
            Whether to use Theano or TensorFlow data format
            in inputs/kernels/outputs.
        dilation_rate: tuple of 2 integers.

    # Raises
        ValueError: if using an even kernel size with padding 'same'.
    """
    ...

def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """1D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: strides integer.
        padding: string, `"same"` or `"valid"`.
        data_format: string, `"channels_last"` or `"channels_first"`.
        dilation_rate: integer dilation rate.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `data_format` is neither `"channels_last"` or
        `"channels_first"`.
    """
    ...

def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """2D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: strides tuple (length 2).
        padding: string, `"same"` or `"valid"`.
        data_format: string, `"channels_last"` or `"channels_first"`.
        dilation_rate: tuple of integers,
            dilation rates for the separable convolution.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `data_format` is neither `"channels_last"` or
        `"channels_first"`.
    """
    ...

def depthwise_conv2d(x, depthwise_kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """2D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        strides: strides tuple (length 2).
        padding: string, `"same"` or `"valid"`.
        data_format: string, `"channels_last"` or `"channels_first"`.
        dilation_rate: tuple of integers,
            dilation rates for the separable convolution.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `data_format` is neither `"channels_last"` or
        `"channels_first"`.
    """
    ...

def conv3d(x, kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    """3D convolution.

    # Arguments
        kernel: kernel tensor.
        strides: strides tuple.
        padding: string, "same" or "valid".
        data_format: "channels_last" or "channels_first".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.
    """
    ...

def conv3d_transpose(x, kernel, output_shape, strides=..., padding=..., data_format: Optional[Any] = ...):
    """3D deconvolution (transposed convolution).

    # Arguments
        kernel: kernel tensor.
        output_shape: desired dimensions of output.
        strides: strides tuple.
        padding: string, "same" or "valid".
        data_format: "channels_last" or "channels_first".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.

    # Raises
        ValueError: if using an even kernel size with padding 'same'.
    """
    ...

def pool2d(x, pool_size, strides=..., padding=..., data_format: Optional[Any] = ..., pool_mode=...):
    ...

def pool3d(x, pool_size, strides=..., padding=..., data_format: Optional[Any] = ..., pool_mode=...):
    ...

def bias_add(x, bias, data_format: Optional[Any] = ...):
    ...

def random_normal(shape, mean=..., stddev=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def random_uniform(shape, minval=..., maxval=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def random_binomial(shape, p=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def truncated_normal(shape, mean=..., stddev=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def ctc_interleave_blanks(Y):
    ...

def ctc_create_skip_idxs(Y):
    ...

def ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev):
    ...

def ctc_path_probs(predict, Y, alpha=...):
    ...

def ctc_cost(predict, Y):
    ...

def ctc_batch_cost(y_true, y_pred, input_length, label_length):
    """Runs CTC loss algorithm on each batch element.

    # Arguments
        y_true: tensor (samples, max_string_length) containing the truth labels
        y_pred: tensor (samples, time_steps, num_categories) containing the
                prediction, or output of the softmax
        input_length: tensor (samples,1) containing the sequence length for
                each batch item in y_pred
        label_length: tensor (samples,1) containing the sequence length for
                each batch item in y_true

    # Returns
        Tensor with shape (samples,1) containing the
            CTC loss of each element
    """
    ...

def map_fn(fn, elems, name: Optional[Any] = ..., dtype: Optional[Any] = ...):
    """Map the function fn over the elements elems and return the outputs.

    # Arguments
        fn: Callable that will be called upon each element in elems
        elems: tensor, at least 2 dimensional
        name: A string name for the map node in the graph

    # Returns
        Tensor with first dimension equal to the elems and second depending on
        fn
    """
    ...

def foldl(fn, elems, initializer: Optional[Any] = ..., name: Optional[Any] = ...):
    """Reduce elems using fn to combine them from left to right.

    # Arguments
        fn: Callable that will be called upon each element in elems and an
            accumulator, for instance lambda acc, x: acc + x
        elems: tensor
        initializer: The first value used (elems[0] in case of None)
        name: A string name for the foldl node in the graph

    # Returns
        Same type and shape as initializer
    """
    ...

def foldr(fn, elems, initializer: Optional[Any] = ..., name: Optional[Any] = ...):
    """Reduce elems using fn to combine them from right to left.

    # Arguments
        fn: Callable that will be called upon each element in elems and an
            accumulator, for instance lambda acc, x: acc + x
        elems: tensor
        initializer: The first value used (elems[-1] in case of None)
        name: A string name for the foldr node in the graph

    # Returns
        Same type and shape as initializer
    """
    ...

def local_conv1d(inputs, kernel, kernel_size, strides, data_format: Optional[Any] = ...):
    ...

def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format: Optional[Any] = ...):
    ...

def ctc_label_dense_to_sparse(labels, label_lengths):
    ...

def ctc_decode(y_pred, input_length, greedy: bool = ..., beam_width=..., top_paths=..., merge_repeated: bool = ...):
    ...

def control_dependencies(control_inputs):
    ...

