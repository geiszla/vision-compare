"""
This type stub file was generated by pyright.
"""

import cntk as C
import numpy as np
from collections import defaultdict
from contextlib import contextmanager
from typing import Any, Optional

b_any = any
py_slice = slice
dev = C.device.use_default_device()
if dev.type() == 0:
    ...
_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1, name='_keras_learning_phase')
_LEARNING_PHASE = - 1
_UID_PREFIXES = defaultdict(int)
grad_parameter_dict = {  }
NAME_SCOPE_STACK = []
@contextmanager
def name_scope(name):
    ...

def get_uid(prefix=...):
    ...

def learning_phase():
    ...

def set_learning_phase(value):
    ...

def clear_session():
    """Reset learning phase flag for cntk backend.
    """
    ...

def in_train_phase(x, alt, training: Optional[Any] = ...):
    ...

def in_test_phase(x, alt, training: Optional[Any] = ...):
    ...

def _convert_string_dtype(dtype):
    ...

def _convert_dtype_string(dtype):
    ...

def variable(value, dtype: Optional[Any] = ..., name: Optional[Any] = ..., constraint: Optional[Any] = ...):
    """Instantiates a variable and returns it.

    # Arguments
        value: Numpy array, initial value of the tensor.
        dtype: Tensor type.
        name: Optional name string for the tensor.
        constraint: Optional projection function to be
            applied to the variable after an optimizer update.

    # Returns
        A variable instance (with Keras metadata included).
    """
    ...

def is_variable(x):
    ...

def bias_add(x, bias, data_format: Optional[Any] = ...):
    ...

def eval(x):
    ...

def placeholder(shape: Optional[Any] = ..., ndim: Optional[Any] = ..., dtype: Optional[Any] = ..., sparse: bool = ..., name: Optional[Any] = ..., dynamic_axis_num=...):
    ...

def is_placeholder(x):
    """Returns whether `x` is a placeholder.

    # Arguments
        x: A candidate placeholder.

    # Returns
        Boolean.
    """
    ...

def is_keras_tensor(x):
    ...

def is_tensor(x):
    ...

def shape(x):
    ...

def is_sparse(tensor):
    ...

def int_shape(x):
    ...

def ndim(x):
    ...

def _prepare_name(name, default):
    ...

def constant(value, dtype: Optional[Any] = ..., shape: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def random_binomial(shape, p=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def random_uniform(shape, minval=..., maxval=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def random_uniform_variable(shape, low, high, dtype: Optional[Any] = ..., name: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def random_normal_variable(shape, mean, scale, dtype: Optional[Any] = ..., name: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def random_normal(shape, mean=..., stddev=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def truncated_normal(shape, mean=..., stddev=..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def dtype(x):
    ...

def zeros(shape, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def ones(shape, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def eye(size, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def zeros_like(x, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def ones_like(x, dtype: Optional[Any] = ..., name: Optional[Any] = ...):
    ...

def count_params(x):
    ...

def cast(x, dtype):
    ...

def size(x, name: Optional[Any] = ...):
    ...

def dot(x, y):
    ...

def batch_dot(x, y, axes: Optional[Any] = ...):
    ...

def transpose(x):
    ...

def gather(reference, indices):
    ...

def _remove_dims(x, axis, keepdims: bool = ...):
    ...

def max(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def min(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def sum(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def prod(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def logsumexp(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def var(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def std(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def expand_dims(x, axis=...):
    ...

def squeeze(x, axis):
    ...

def tile(x, n):
    ...

def _normalize_axis(axis, x):
    ...

def _reshape_dummy_dim(x, axis):
    ...

def mean(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def any(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def all(x, axis: Optional[Any] = ..., keepdims: bool = ...):
    ...

def classification_error(target, output, axis=...):
    ...

def argmax(x, axis=...):
    ...

def argmin(x, axis=...):
    ...

def square(x):
    ...

def abs(x):
    ...

def sqrt(x):
    ...

def exp(x):
    ...

def log(x):
    ...

def round(x):
    ...

def sigmoid(x):
    ...

def sign(x):
    ...

def pow(x, a):
    ...

def clip(x, min_value, max_value):
    ...

def binary_crossentropy(target, output, from_logits: bool = ...):
    ...

def get_variable_shape(x):
    ...

def update(x, new_x):
    ...

def moving_average_update(variable, value, momentum):
    ...

def update_add(x, increment):
    ...

def update_sub(x, decrement):
    ...

def gradients(loss, variables):
    ...

def equal(x, y):
    ...

def not_equal(x, y):
    ...

def greater(x, y):
    ...

def greater_equal(x, y):
    ...

def less(x, y):
    ...

def less_equal(x, y):
    ...

def maximum(x, y):
    ...

def minimum(x, y):
    ...

def sin(x):
    ...

def cos(x):
    ...

def normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=...):
    ...

def _moments(x, axes: Optional[Any] = ..., shift: Optional[Any] = ..., keep_dims: bool = ...):
    ...

def batch_normalization(x, mean, var, beta, gamma, axis=..., epsilon=...):
    ...

def concatenate(tensors, axis=...):
    ...

def stack(x, axis=...):
    ...

def flatten(x):
    ...

def reshape(x, shape):
    ...

def permute_dimensions(x, pattern):
    ...

def resize_images(x, height_factor, width_factor, data_format, interpolation=...):
    ...

def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):
    ...

def repeat_elements(x, rep, axis):
    ...

def repeat(x, n):
    ...

def tanh(x):
    ...

def _static_rnn(step_function, inputs, initial_states, go_backwards: bool = ..., mask: Optional[Any] = ..., constants: Optional[Any] = ..., unroll: bool = ..., input_length: Optional[Any] = ...):
    ...

def rnn(step_function, inputs, initial_states, go_backwards: bool = ..., mask: Optional[Any] = ..., constants: Optional[Any] = ..., unroll: bool = ..., input_length: Optional[Any] = ...):
    ...

def has_seq_axis(x):
    ...

def l2_normalize(x, axis: Optional[Any] = ...):
    ...

def hard_sigmoid(x):
    ...

def conv1d(x, kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def conv2d(x, kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def depthwise_conv2d(x, depthwise_kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def conv3d(x, kernel, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def conv3d_transpose(x, kernel, output_shape, strides=..., padding=..., data_format: Optional[Any] = ...):
    ...

def pool2d(x, pool_size, strides=..., padding=..., data_format: Optional[Any] = ..., pool_mode=...):
    ...

def pool3d(x, pool_size, strides=..., padding=..., data_format: Optional[Any] = ..., pool_mode=...):
    ...

def relu(x, alpha=..., max_value: Optional[Any] = ..., threshold=...):
    ...

def dropout(x, level, noise_shape: Optional[Any] = ..., seed: Optional[Any] = ...):
    ...

def batch_flatten(x):
    ...

def softmax(x, axis=...):
    ...

def softplus(x):
    ...

def softsign(x):
    ...

def categorical_crossentropy(target, output, from_logits: bool = ..., axis=...):
    ...

def sparse_categorical_crossentropy(target, output, from_logits: bool = ..., axis=...):
    ...

class Function(object):
    def __init__(self, inputs, outputs, updates=..., **kwargs):
        self.placeholders = ...
        self.trainer = ...
        self.unrelated_updates = ...
        self.updates = ...
    
    @staticmethod
    def _is_input_shape_compatible(input, placeholder):
        ...
    
    def __call__(self, inputs):
        ...
    


def function(inputs, outputs, updates=..., **kwargs):
    ...

def temporal_padding(x, padding=...):
    ...

def _padding(x, pattern, axis):
    ...

def pad(x, pad_info, data_format, num_dynamic_axis):
    ...

def spatial_2d_padding(x, padding=..., data_format: Optional[Any] = ...):
    ...

def spatial_3d_padding(x, padding=..., data_format: Optional[Any] = ...):
    ...

def one_hot(indices, num_classes):
    ...

def get_value(x):
    ...

def batch_get_value(xs):
    ...

def set_value(x, value):
    ...

def print_tensor(x, message=...):
    ...

def batch_set_value(tuples):
    ...

def stop_gradient(variables):
    ...

def switch(condition, then_expression, else_expression):
    ...

def elu(x, alpha=...):
    ...

def in_top_k(predictions, targets, k):
    ...

def conv2d_transpose(x, kernel, output_shape, strides=..., padding=..., data_format: Optional[Any] = ..., dilation_rate=...):
    ...

def identity(x, name: Optional[Any] = ...):
    ...

def _preprocess_conv2d_input(x, data_format):
    ...

def _preprocess_conv2d_kernel(kernel, data_format):
    ...

def _preprocess_border_mode(padding):
    ...

def _postprocess_conv2d_output(x, data_format):
    ...

def _preprocess_conv3d_input(x, data_format):
    ...

def _preprocess_conv3d_kernel(kernel, dim_ordering):
    ...

def _postprocess_conv3d_output(x, dim_ordering):
    ...

def _get_dynamic_axis_num(x):
    ...

def _contain_seqence_axis(x):
    ...

def get_num_dynamic_axis(x):
    ...

def _reduce_on_axis(x, axis, reduce_fun_name):
    ...

def _reshape_sequence(x, time_step):
    ...

def local_conv1d(inputs, kernel, kernel_size, strides, data_format: Optional[Any] = ...):
    ...

def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format: Optional[Any] = ...):
    ...

def reverse(x, axes):
    ...

def slice(x, start, size):
    ...

def _reshape_batch(x, shape):
    ...

def _get_cntk_version():
    ...

class ReshapeBatch(C.ops.functions.UserFunction):
    def __init__(self, input, shape, name=...):
        self.from_shape = ...
        self.target_shape = ...
    
    def infer_outputs(self):
        ...
    
    def forward(self, arguments, device: Optional[Any] = ..., outputs_to_retain: Optional[Any] = ...):
        ...
    
    def backward(self, state, root_gradients):
        ...
    


class ConvertToBatch(C.ops.functions.UserFunction):
    """Converts input first axis to CNTK batch axis.

    We may introduce this operation in CNTK native
    implementation later.

    # Arguments
        inputs: a cntk variable (parameter/constant)
        name: name of this node
    """
    def __init__(self, input, name=...):
        ...
    
    def infer_outputs(self):
        ...
    
    def forward(self, arguments, device: Optional[Any] = ..., outputs_to_retain: Optional[Any] = ...):
        ...
    
    def backward(self, state, root_gradients):
        ...
    


class ConvertToStatic(C.ops.functions.UserFunction):
    """Converts input first axis to CNTK static axis.

    We may introduce this operation in CNTK native
    implementation later.

    # Arguments
        inputs: a cntk tensor which has batch axis
        batch_size: size of batch axis.
        name: name of this node.
    """
    def __init__(self, input, batch_size, name=...):
        self.target_shape = ...
    
    def infer_outputs(self):
        ...
    
    def forward(self, arguments, device: Optional[Any] = ..., outputs_to_retain: Optional[Any] = ...):
        ...
    
    def backward(self, state, root_gradients):
        ...
    


class LambdaFunc(C.ops.functions.UserFunction):
    def __init__(self, arg, when=..., execute=..., name=...):
        self.when = ...
        self.execute = ...
    
    def infer_outputs(self):
        ...
    
    def forward(self, argument, device: Optional[Any] = ..., outputs_to_retain: Optional[Any] = ...):
        ...
    
    def backward(self, state, root_gradients):
        ...
    


def reset_uids():
    ...

def to_dense(tensor):
    ...

def cumsum(x, axis=...):
    ...

def cumprod(x, axis=...):
    ...

def arange(start, stop: Optional[Any] = ..., step=..., dtype=...):
    ...

def ctc_label_dense_to_sparse(labels, label_lengths):
    ...

def ctc_batch_cost(y_true, y_pred, input_length, label_length):
    ...

def ctc_decode(y_pred, input_length, greedy: bool = ..., beam_width=..., top_paths=..., merge_repeated: bool = ...):
    ...

def map_fn(fn, elems, name: Optional[Any] = ..., dtype: Optional[Any] = ...):
    ...

def foldl(fn, elems, initializer: Optional[Any] = ..., name: Optional[Any] = ...):
    """Reduce `elems` by `fn` combined them from left to right on dimension 0.

    # Arguments
        fn: Callable that will be called upon each element in `elems`
            (and on the optional `initializer`) passed as a second argument.
            The first argument passed to `fn` is the accumulator which is the
            accumulated value calculated from the preceding invocation of `fn`.
            Example For `fn`:
            ```python
            lambda acc, x: acc + x
            ```
        elems: Tensor
        initializer: (optional) Tensor, the initial value for the accumulator.
            In case of None value is provided during the call
            the first value is used (`elems[0]`) as `initializer` from `elems`
        name: (optional) String, name for the foldl node in the graph.

    # Returns
        Same type and shape as `initializer`

    # Raises:
        TypeError: if `fn` is not callable.
        TypeError: if `initializer` is neither a tensor nor None value.
        TypeError: if `elems` is not a tensor.
    """
    ...

def foldr(fn, elems, initializer: Optional[Any] = ..., name: Optional[Any] = ...):
    """Reduce `elems` by `fn` combined them from right to left on dimension 0.

    # Arguments
        fn: Callable that will be called upon each element in `elems`
            (and on the optional `initializer`) passed as a second argument.
            The first argument passed to `fn` is the accumulator which is the
            accumulated value calculated from the preceding invocation of `fn`.
            Example For `fn`:
            ```python
            lambda acc, x: acc + x
            ```
        elems: Tensor
        initializer: (optional) Tensor, the initial value for the accumulator.
            In case of None value is provided during the call
            the last value is used (`elems[-1]`) as `initializer` from `elems`
        name: (optional) String, name for the foldr node in the graph.

    # Returns
        Same type and shape as `initializer`

    # Raises:
        TypeError: if `fn` is not callable.
        TypeError: if `initializer` is neither a tensor nor None value.
        TypeError: if `elems` is not a tensor.
    """
    ...

def control_dependencies(control_inputs):
    ...

