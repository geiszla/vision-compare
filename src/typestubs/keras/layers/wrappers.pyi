"""
This type stub file was generated by pyright.
"""

from ..engine.base_layer import Layer, disable_tracking
from typing import Any, Optional

"""Layers that augment the functionality of a base layer.
"""
class Wrapper(Layer):
    """Abstract wrapper base class.

    Wrappers take another layer and augment it in various ways.
    Do not use this class as a layer, it is only an abstract base class.
    Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.

    # Arguments
        layer: The layer to be wrapped.
    """
    @disable_tracking
    def __init__(self, layer, **kwargs):
        self.layer = ...
    
    def build(self, input_shape: Optional[Any] = ...):
        self.built = ...
    
    @property
    def activity_regularizer(self):
        ...
    
    @property
    def trainable(self):
        ...
    
    @trainable.setter
    def trainable(self, value):
        ...
    
    @property
    def trainable_weights(self):
        ...
    
    @property
    def non_trainable_weights(self):
        ...
    
    @property
    def updates(self):
        ...
    
    def get_updates_for(self, inputs: Optional[Any] = ...):
        ...
    
    @property
    def losses(self):
        ...
    
    def get_losses_for(self, inputs: Optional[Any] = ...):
        ...
    
    def get_weights(self):
        ...
    
    def set_weights(self, weights):
        ...
    
    def get_config(self):
        ...
    
    @classmethod
    def from_config(cls, config, custom_objects: Optional[Any] = ...):
        ...
    


class TimeDistributed(Wrapper):
    """This wrapper applies a layer to every temporal slice of an input.

    The input should be at least 3D, and the dimension of index one
    will be considered to be the temporal dimension.

    Consider a batch of 32 samples,
    where each sample is a sequence of 10 vectors of 16 dimensions.
    The batch input shape of the layer is then `(32, 10, 16)`,
    and the `input_shape`, not including the samples dimension, is `(10, 16)`.

    You can then use `TimeDistributed` to apply a `Dense` layer
    to each of the 10 timesteps, independently:

    ```python
        # as the first layer in a model
        model = Sequential()
        model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
        # now model.output_shape == (None, 10, 8)
    ```

    The output will then have shape `(32, 10, 8)`.

    In subsequent layers, there is no need for the `input_shape`:

    ```python
        model.add(TimeDistributed(Dense(32)))
        # now model.output_shape == (None, 10, 32)
    ```

    The output will then have shape `(32, 10, 32)`.

    `TimeDistributed` can be used with arbitrary layers, not just `Dense`,
    for instance with a `Conv2D` layer:

    ```python
        model = Sequential()
        model.add(TimeDistributed(Conv2D(64, (3, 3)),
                                  input_shape=(10, 299, 299, 3)))
    ```

    # Arguments
        layer: a layer instance.
    """
    def __init__(self, layer, **kwargs):
        self.supports_masking = ...
    
    def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape: Optional[Any] = ...):
        """Finds non-specific dimensions in the static shapes
        and replaces them by the corresponding dynamic shapes of the tensor.

        # Arguments
            init_tuple: a tuple, the first part of the output shape
            tensor: the tensor from which to get the (static and dynamic) shapes
                as the last part of the output shape
            start_idx: int, which indicate the first dimension to take from
                the static shape of the tensor
            int_shape: an alternative static shape to take as the last part
                of the output shape

        # Returns
            The new int_shape with the first part from init_tuple
            and the last part from either `int_shape` (if provided)
            or K.int_shape(tensor), where every `None` is replaced by
            the corresponding dimension from K.shape(tensor)
        """
        ...
    
    def build(self, input_shape):
        self.input_spec = ...
    
    def compute_output_shape(self, input_shape):
        ...
    
    def call(self, inputs, training: Optional[Any] = ..., mask: Optional[Any] = ...):
        ...
    
    def compute_mask(self, inputs, mask: Optional[Any] = ...):
        """Computes an output mask tensor for Embedding layer
        based on the inputs, mask, and the inner layer.

        If batch size is specified:
        Simply return the input `mask`. (An rnn-based implementation with
        more than one rnn inputs is required but not supported in Keras yet.)

        Otherwise we call `compute_mask` of the inner layer at each time step.
        If the output mask at each time step is not `None`:
        (E.g., inner layer is Masking or RNN)
        Concatenate all of them and return the concatenation.
        If the output mask at each time step is `None` and
        the input mask is not `None`:
        (E.g., inner layer is Dense)
        Reduce the input_mask to 2 dimensions and return it.
        Otherwise (both the output mask and the input mask are `None`):
        (E.g., `mask` is not used at all)
        Return `None`.

        # Arguments
            inputs: Tensor
            mask: Tensor
        # Returns
            None or a tensor
        """
        ...
    


class Bidirectional(Wrapper):
    """Bidirectional wrapper for RNNs.

    # Arguments
        layer: `Recurrent` instance.
        merge_mode: Mode by which outputs of the
            forward and backward RNNs will be combined.
            One of {'sum', 'mul', 'concat', 'ave', None}.
            If None, the outputs will not be combined,
            they will be returned as a list.
        weights: Initial weights to load in the Bidirectional model

    # Raises
        ValueError: In case of invalid `merge_mode` argument.

    # Examples

    ```python
        model = Sequential()
        model.add(Bidirectional(LSTM(10, return_sequences=True),
                                input_shape=(5, 10)))
        model.add(Bidirectional(LSTM(10)))
        model.add(Dense(5))
        model.add(Activation('softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    ```
    """
    def __init__(self, layer, merge_mode=..., weights: Optional[Any] = ..., **kwargs):
        self.merge_mode = ...
        self.stateful = ...
        self.return_sequences = ...
        self.return_state = ...
        self.supports_masking = ...
        self.input_spec = ...
    
    @disable_tracking
    def _set_sublayers(self, layer):
        self.forward_layer = ...
        self.backward_layer = ...
    
    @property
    def trainable(self):
        ...
    
    @trainable.setter
    def trainable(self, value):
        ...
    
    def get_weights(self):
        ...
    
    def set_weights(self, weights):
        ...
    
    def compute_output_shape(self, input_shape):
        ...
    
    def __call__(self, inputs, initial_state: Optional[Any] = ..., constants: Optional[Any] = ..., **kwargs):
        ...
    
    def call(self, inputs, mask: Optional[Any] = ..., training: Optional[Any] = ..., initial_state: Optional[Any] = ..., constants: Optional[Any] = ...):
        ...
    
    def reset_states(self):
        ...
    
    def build(self, input_shape):
        self.built = ...
    
    def compute_mask(self, inputs, mask):
        ...
    
    @property
    def trainable_weights(self):
        ...
    
    @property
    def non_trainable_weights(self):
        ...
    
    @property
    def updates(self):
        ...
    
    def get_updates_for(self, inputs: Optional[Any] = ...):
        ...
    
    @property
    def losses(self):
        ...
    
    def get_losses_for(self, inputs: Optional[Any] = ...):
        ...
    
    @property
    def constraints(self):
        ...
    
    def get_config(self):
        ...
    
    @classmethod
    def from_config(cls, config, custom_objects: Optional[Any] = ...):
        ...
    


