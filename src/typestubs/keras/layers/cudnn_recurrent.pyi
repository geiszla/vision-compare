"""
This type stub file was generated by pyright.
"""

from .recurrent import RNN
from typing import Any, Optional

"""Recurrent layers backed by cuDNN.
"""
class _CuDNNRNN(RNN):
    """Private base class for CuDNNGRU and CuDNNLSTM.

    # Arguments
        return_sequences: Boolean. Whether to return the last output.
            in the output sequence, or the full sequence.
        return_state: Boolean. Whether to return the last state
            in addition to the output.
        stateful: Boolean (default False). If True, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
    """
    def __init__(self, return_sequences: bool = ..., return_state: bool = ..., go_backwards: bool = ..., stateful: bool = ..., **kwargs):
        self.return_sequences = ...
        self.return_state = ...
        self.go_backwards = ...
        self.stateful = ...
        self.supports_masking = ...
        self.input_spec = ...
        self.state_spec = ...
        self.constants_spec = ...
    
    def _canonical_to_params(self, weights, biases):
        ...
    
    def call(self, inputs, mask: Optional[Any] = ..., training: Optional[Any] = ..., initial_state: Optional[Any] = ...):
        ...
    
    def get_config(self):
        ...
    
    @classmethod
    def from_config(cls, config):
        ...
    
    @property
    def trainable_weights(self):
        ...
    
    @property
    def non_trainable_weights(self):
        ...
    
    @property
    def losses(self):
        ...
    
    def get_losses_for(self, inputs: Optional[Any] = ...):
        ...
    


class CuDNNGRU(_CuDNNRNN):
    """Fast GRU implementation backed by [CuDNN](https://developer.nvidia.com/cudnn).

    Can only be run on GPU, with the TensorFlow backend.

    # Arguments
        units: Positive integer, dimensionality of the output space.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
            (see [initializers](../initializers.md)).
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix,
            used for the linear transformation of the recurrent state.
            (see [initializers](../initializers.md)).
        bias_initializer: Initializer for the bias vector
            (see [initializers](../initializers.md)).
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        recurrent_regularizer: Regularizer function applied to
            the `recurrent_kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        bias_regularizer: Regularizer function applied to the bias vector
            (see [regularizer](../regularizers.md)).
        activity_regularizer: Regularizer function applied to
            the output of the layer (its "activation").
            (see [regularizer](../regularizers.md)).
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix
            (see [constraints](../constraints.md)).
        recurrent_constraint: Constraint function applied to
            the `recurrent_kernel` weights matrix
            (see [constraints](../constraints.md)).
        bias_constraint: Constraint function applied to the bias vector
            (see [constraints](../constraints.md)).
        return_sequences: Boolean. Whether to return the last output.
            in the output sequence, or the full sequence.
        return_state: Boolean. Whether to return the last state
            in addition to the output.
        stateful: Boolean (default False). If True, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
    """
    def __init__(self, units, kernel_initializer=..., recurrent_initializer=..., bias_initializer=..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., activity_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., return_sequences: bool = ..., return_state: bool = ..., stateful: bool = ..., **kwargs):
        self.units = ...
        self.kernel_initializer = ...
        self.recurrent_initializer = ...
        self.bias_initializer = ...
        self.kernel_regularizer = ...
        self.recurrent_regularizer = ...
        self.bias_regularizer = ...
        self.activity_regularizer = ...
        self.kernel_constraint = ...
        self.recurrent_constraint = ...
        self.bias_constraint = ...
    
    @property
    def cell(self):
        ...
    
    def build(self, input_shape):
        self.kernel = ...
        self.recurrent_kernel = ...
        self.bias = ...
        self.kernel_z = ...
        self.recurrent_kernel_z = ...
        self.kernel_r = ...
        self.recurrent_kernel_r = ...
        self.kernel_h = ...
        self.recurrent_kernel_h = ...
        self.bias_z_i = ...
        self.bias_r_i = ...
        self.bias_h_i = ...
        self.bias_z = ...
        self.bias_r = ...
        self.bias_h = ...
        self.built = ...
    
    def _process_batch(self, inputs, initial_state):
        ...
    
    def get_config(self):
        ...
    


class CuDNNLSTM(_CuDNNRNN):
    """Fast LSTM implementation with [CuDNN](https://developer.nvidia.com/cudnn).

    Can only be run on GPU, with the TensorFlow backend.

    # Arguments
        units: Positive integer, dimensionality of the output space.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
            (see [initializers](../initializers.md)).
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix,
            used for the linear transformation of the recurrent state.
            (see [initializers](../initializers.md)).
        bias_initializer: Initializer for the bias vector
            (see [initializers](../initializers.md)).
        unit_forget_bias: Boolean.
            If True, add 1 to the bias of the forget gate at initialization.
            Setting it to true will also force `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al. (2015)](
            http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        recurrent_regularizer: Regularizer function applied to
            the `recurrent_kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        bias_regularizer: Regularizer function applied to the bias vector
            (see [regularizer](../regularizers.md)).
        activity_regularizer: Regularizer function applied to
            the output of the layer (its "activation").
            (see [regularizer](../regularizers.md)).
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix
            (see [constraints](../constraints.md)).
        recurrent_constraint: Constraint function applied to
            the `recurrent_kernel` weights matrix
            (see [constraints](../constraints.md)).
        bias_constraint: Constraint function applied to the bias vector
            (see [constraints](../constraints.md)).
        return_sequences: Boolean. Whether to return the last output.
            in the output sequence, or the full sequence.
        return_state: Boolean. Whether to return the last state
            in addition to the output.
        stateful: Boolean (default False). If True, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
    """
    def __init__(self, units, kernel_initializer=..., recurrent_initializer=..., bias_initializer=..., unit_forget_bias: bool = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., activity_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., return_sequences: bool = ..., return_state: bool = ..., stateful: bool = ..., **kwargs):
        self.units = ...
        self.kernel_initializer = ...
        self.recurrent_initializer = ...
        self.bias_initializer = ...
        self.unit_forget_bias = ...
        self.kernel_regularizer = ...
        self.recurrent_regularizer = ...
        self.bias_regularizer = ...
        self.activity_regularizer = ...
        self.kernel_constraint = ...
        self.recurrent_constraint = ...
        self.bias_constraint = ...
    
    @property
    def cell(self):
        ...
    
    def build(self, input_shape):
        self.kernel = ...
        self.recurrent_kernel = ...
        self.bias = ...
        self.kernel_i = ...
        self.kernel_f = ...
        self.kernel_c = ...
        self.kernel_o = ...
        self.recurrent_kernel_i = ...
        self.recurrent_kernel_f = ...
        self.recurrent_kernel_c = ...
        self.recurrent_kernel_o = ...
        self.bias_i_i = ...
        self.bias_f_i = ...
        self.bias_c_i = ...
        self.bias_o_i = ...
        self.bias_i = ...
        self.bias_f = ...
        self.bias_c = ...
        self.bias_o = ...
        self.built = ...
    
    def _process_batch(self, inputs, initial_state):
        ...
    
    def get_config(self):
        ...
    


