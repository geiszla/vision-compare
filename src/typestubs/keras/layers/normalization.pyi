"""
This type stub file was generated by pyright.
"""

from ..engine.base_layer import Layer
from ..legacy import interfaces
from typing import Any, Optional

"""Normalization layers.
"""
class BatchNormalization(Layer):
    """Batch normalization layer (Ioffe and Szegedy, 2014).

    Normalize the activations of the previous layer at each batch,
    i.e. applies a transformation that maintains the mean activation
    close to 0 and the activation standard deviation close to 1.

    # Arguments
        axis: Integer, the axis that should be normalized
            (typically the features axis).
            For instance, after a `Conv2D` layer with
            `data_format="channels_first"`,
            set `axis=1` in `BatchNormalization`.
        momentum: Momentum for the moving mean and the moving variance.
        epsilon: Small float added to variance to avoid dividing by zero.
        center: If True, add offset of `beta` to normalized tensor.
            If False, `beta` is ignored.
        scale: If True, multiply by `gamma`.
            If False, `gamma` is not used.
            When the next layer is linear (also e.g. `nn.relu`),
            this can be disabled since the scaling
            will be done by the next layer.
        beta_initializer: Initializer for the beta weight.
        gamma_initializer: Initializer for the gamma weight.
        moving_mean_initializer: Initializer for the moving mean.
        moving_variance_initializer: Initializer for the moving variance.
        beta_regularizer: Optional regularizer for the beta weight.
        gamma_regularizer: Optional regularizer for the gamma weight.
        beta_constraint: Optional constraint for the beta weight.
        gamma_constraint: Optional constraint for the gamma weight.

    # Input shape
        Arbitrary. Use the keyword argument `input_shape`
        (tuple of integers, does not include the samples axis)
        when using this layer as the first layer in a model.

    # Output shape
        Same shape as input.

    # References
        - [Batch Normalization: Accelerating Deep Network Training by
           Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
    """
    @interfaces.legacy_batchnorm_support
    def __init__(self, axis=..., momentum=..., epsilon=..., center: bool = ..., scale: bool = ..., beta_initializer=..., gamma_initializer=..., moving_mean_initializer=..., moving_variance_initializer=..., beta_regularizer: Optional[Any] = ..., gamma_regularizer: Optional[Any] = ..., beta_constraint: Optional[Any] = ..., gamma_constraint: Optional[Any] = ..., **kwargs):
        self.supports_masking = ...
        self.axis = ...
        self.momentum = ...
        self.epsilon = ...
        self.center = ...
        self.scale = ...
        self.beta_initializer = ...
        self.gamma_initializer = ...
        self.moving_mean_initializer = ...
        self.moving_variance_initializer = ...
        self.beta_regularizer = ...
        self.gamma_regularizer = ...
        self.beta_constraint = ...
        self.gamma_constraint = ...
    
    def build(self, input_shape):
        self.input_spec = ...
        self.moving_mean = ...
        self.moving_variance = ...
        self.built = ...
    
    def call(self, inputs, training: Optional[Any] = ...):
        ...
    
    def get_config(self):
        ...
    
    def compute_output_shape(self, input_shape):
        ...
    


